# -*- coding: utf-8 -*-
"""ImageStitching

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VgBCGdz6B5OLgdkhwxNnoF6dRRNBv0vS

### Image Stitching - EE604 Project by Shashank Gupta and Sanket Anand
The code below is a partial reproduction of the algorithm described in the following paper:

Brown, Matthew, and David G. Lowe. "Automatic panoramic image stitching using invariant features." International journal of computer vision 74.1 (2007): 59-73.
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd drive/My\ Drive/ImageStitching

from PIL import Image
import cv2 as cv
import numpy as np
import os
import timeit
import imutils

MIN_MATCH_COUNT = 4
NUM = 7
SHIFTING = False
CYLINDRICAL_WARP = False
B_WEIGHT = 0.5

# dataset = "LunchRoom"
# data_path = "data" + os.sep + dataset

def fetch_images(data_path, num=-1):
  images = []
  for filename in sorted(os.listdir(data_path)): # sorting may not always work. Adjust according to dataset. Or implement generalised method in Lowe
    if filename.endswith(".jpg") or filename.endswith(".bmp"):
      img = cv.imread(data_path + os.sep + filename)
      img = imutils.resize(img, width=400)
      if CYLINDRICAL_WARP:
        img = cylindricalWarp(img)
      images.append(img)
      # TODO: PIL may be faster. 
  return images

def extract_features(images, method):
  features = []
  sift = cv.xfeatures2d.SIFT_create()
  surf = cv.xfeatures2d.SURF_create()
  orb = cv.ORB_create()
  for img in images:
    grey = cv.cvtColor(img, cv.COLOR_BGR2GRAY)
    if method is "sift":
      kp, des = sift.detectAndCompute(grey, None)
    elif method is "surf":
      kp, des = surf.detectAndCompute(grey, None)
    elif method is "orb":
      kp, des = orb.detectAndCompute(grey, None)
    features.append((kp, des))
  return features

def lowe_ratio_test(all_matches, lowe_ratio):
  good_matches = []
  for m, n in all_matches:
    if m.distance < lowe_ratio * n.distance:
      good_matches.append(m)
  return good_matches

def find_matches(des_a, des_b, f_method, matcher, lowe_ratio):
  if matcher is "brute_force":

    if f_method is "sift" or f_method is "surf":
      bf = cv.BFMatcher()
      all_matches = bf.knnMatch(des_a, des_b, k = 2)
      good_matches = lowe_ratio_test(all_matches, lowe_ratio)

    elif f_method is "orb":
      bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)
      all_matches = bf.knnMatch(des_a, des_b, k = 2)
      good_matches = lowe_ratio_test(all_matches, lowe_ratio)

  elif matcher is "flann":

    search_params = {}  # could also set this as dict(checks=50) or dict(checks=100)
    if f_method is "sift" or f_method is "surf":
      FLANN_INDEX_KDTREE = 0
      index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)
    elif f_method is "orb":
      FLANN_INDEX_LSH = 6
      index_params = dict(algorithm = FLANN_INDEX_LSH, table_number = 6, key_size = 12, multi_probe_level = 1) # 12, 20, 2

    flann = cv.FlannBasedMatcher(index_params, search_params)
    all_matches = flann.knnMatch(des_a, des_b, k = 2)
    good_matches = lowe_ratio_test(all_matches, lowe_ratio)

  return good_matches

def match_keypoints(features, f_method, matcher, lowe_ratio, max_threshold):
  kp_a, des_a = features[0]
  kp_b, des_b = features[1]
  
  good_matches = find_matches(des_a, des_b, f_method, matcher, lowe_ratio)
    
  if len(good_matches) > MIN_MATCH_COUNT: # Otherwise ignore the image, I guess
    # construct the two sets of points
    points_a = np.float32([kp_a[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    points_b = np.float32([kp_b[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    (homography, status) = cv.findHomography(points_a, points_b, cv.RANSAC, max_threshold)
    return (good_matches, homography, status)
  else:
    return None

def left_stitch(images, f_method="sift", matcher="flann", lowe_ratio=0.75, max_threshold=4.0):
  img_a = images[0]
  for img_b in images[1:]:
    features = extract_features([img_b, img_a], f_method)
    matching = match_keypoints(features, f_method, matcher, lowe_ratio, max_threshold)
    if matching is None:
      continue 
    (matches, homography, status) = matching
	# Inverse homography needed as warping the left image. Then blend with offset
    inv_homography = np.linalg.inv(homography)
    start_p = inv_homography @ np.array([0, 0, 1])
    start_p = start_p / start_p[-1]
    inv_homography[0][-1] += abs(round(start_p[0]))
    inv_homography[1][-1] += abs(round(start_p[1]))
    ds = inv_homography @ np.array([img_a.shape[1], img_a.shape[0], 1])
    offsety = int(abs(round(start_p[1])))
    offsetx = int(abs(round(start_p[0])))
    dsize = (int(ds[0]) + offsetx, int(ds[1]) + offsety)
    result_image = cv.warpPerspective(img_a, inv_homography, dsize)
    result_image[offsety:img_b.shape[0] + offsety, offsetx:img_b.shape[1] + offsetx] = img_b
    """blend = B_WEIGHT * result_image[offsety:img_b.shape[0] + offsety, offsetx:img_b.shape[1] + offsetx] + (1 - B_WEIGHT) * img_b
    # blend = blend.astype(int)
    # cv2_imshow(result_image)
    img_b = cv.copyMakeBorder(img_b, offsety, 0, offsetx, 0, cv.BORDER_CONSTANT, value=[0, 0, 0])
    if (img_b.shape[0] > result_image.shape[0]):
      result_image = cv.copyMakeBorder(result_image, 0, img_b.shape[0] - result_image.shape[0], 0, 0, cv.BORDER_CONSTANT, value=[0, 0, 0])
    else:
      img_b = cv.copyMakeBorder(img_b, 0, result_image.shape[0] - img_b.shape[0], 0, 0, cv.BORDER_CONSTANT, value=[0, 0, 0])
    if (img_b.shape[1] > result_image.shape[1]):
      result_image = cv.copyMakeBorder(result_image, 0, 0, 0, img_b.shape[1] - result_image.shape[1], cv.BORDER_CONSTANT, value=[0, 0, 0])
    else:
      img_b = cv.copyMakeBorder(img_b, 0, 0, 0, result_image.shape[1] - img_b.shape[1], cv.BORDER_CONSTANT, value=[0, 0, 0])
    print(img_b.shape, result_image.shape)
    overlap_mask = np.logical_and(img_b, result_image)
    blend = np.where(overlap_mask == True, B_WEIGHT * img_b + (1 - B_WEIGHT) * result_image, img_b)
    blend = np.around(blend).astype(int)
    cv2_imshow(blend)
    result_image = np.where(blend != np.array([0, 0, 0]), blend, result_image)"""
    img_a = result_image
  return img_a

def right_stitch(images, f_method="sift", matcher="flann", lowe_ratio=0.75, max_threshold=4.0):
  img_a = images[0]
  for img_b in images[1:]:
    features = extract_features([img_b, img_a], f_method)
    matching = match_keypoints(features, f_method, matcher, lowe_ratio, max_threshold)
    (matches, homography, status) = matching
    if matching is None:
      continue
    offsety = 0
    if SHIFTING:
      tr_warp = homography @ np.array([img_b.shape[1], 0, 1])
      tr_warp = tr_warp / tr_warp[-1]
      homography[1][-1] += abs(round(tr_warp[1]))
      offsety = int(abs(round(tr_warp[1])))
      img_a = cv.copyMakeBorder(img_a, offsety, 0, 0, 0, cv.BORDER_CONSTANT, value=[0, 0, 0])
    txyz = homography @ np.array([img_b.shape[1], img_b.shape[0], 1])
    txyz = txyz / txyz[-1]
    dsize = (int(txyz[0]) + offsety, int(txyz[1]))
    result_image = cv.warpPerspective(img_b, homography, dsize)
    if (img_a.shape[0] > result_image.shape[0]):
      result_image = cv.copyMakeBorder(result_image, 0, img_a.shape[0] - result_image.shape[0], 0, 0, cv.BORDER_CONSTANT, value=[0, 0, 0])
    else:
      img_a = cv.copyMakeBorder(img_a, 0, result_image.shape[0] - img_a.shape[0], 0, 0, cv.BORDER_CONSTANT, value=[0, 0, 0])
    if (img_a.shape[1] > result_image.shape[1]):
      result_image = cv.copyMakeBorder(result_image, 0, 0, 0, img_a.shape[1] - result_image.shape[1], cv.BORDER_CONSTANT, value=[0, 0, 0])
    else:
      img_a = cv.copyMakeBorder(img_a, 0, 0, 0, result_image.shape[1] - img_a.shape[1], cv.BORDER_CONSTANT, value=[0, 0, 0])
    overlap_mask = np.logical_and(img_a, result_image)
    result_image = np.where(img_a != np.array([0, 0, 0]), img_a, result_image)
    img_a = result_image
  return img_a

def stitch_images(images, f_method="sift", matcher="flann", lowe_ratio=0.75, max_threshold=4.0):
  num_images = len(images)
  left_set = images[:num_images // 2 + 1]
  right_set = images[num_images // 2 + 1:]
  left_stitched = left_stitch(left_set, f_method, matcher, lowe_ratio, max_threshold)
  right_set.insert(0, left_stitched)
  right_stitched = right_stitch(right_set, f_method, matcher, lowe_ratio, max_threshold)
  return right_stitched
